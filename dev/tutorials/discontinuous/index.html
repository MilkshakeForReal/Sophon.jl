<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Fitting a nonlinear discontinuous function · Sophon.jl</title><meta name="title" content="Fitting a nonlinear discontinuous function · Sophon.jl"/><meta property="og:title" content="Fitting a nonlinear discontinuous function · Sophon.jl"/><meta property="twitter:title" content="Fitting a nonlinear discontinuous function · Sophon.jl"/><meta name="description" content="Documentation for Sophon.jl."/><meta property="og:description" content="Documentation for Sophon.jl."/><meta property="twitter:description" content="Documentation for Sophon.jl."/><meta property="og:url" content="https://YichengDWu.github.io/Sophon.jl/tutorials/discontinuous/"/><meta property="twitter:url" content="https://YichengDWu.github.io/Sophon.jl/tutorials/discontinuous/"/><link rel="canonical" href="https://YichengDWu.github.io/Sophon.jl/tutorials/discontinuous/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/indigo.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Sophon.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../ode/">Introduction with Lotka-Volterra System</a></li><li class="is-active"><a class="tocitem" href>Fitting a nonlinear discontinuous function</a><ul class="internal"><li><a class="tocitem" href="#Import-packages"><span>Import packages</span></a></li><li><a class="tocitem" href="#Dataset"><span>Dataset</span></a></li><li><a class="tocitem" href="#Naive-Neural-Networks"><span>Naive Neural Networks</span></a></li><li><a class="tocitem" href="#Siren"><span>Siren</span></a></li><li><a class="tocitem" href="#Gaussian-activation-function"><span>Gaussian activation function</span></a></li><li><a class="tocitem" href="#Quadratic-activation-function"><span>Quadratic activation function</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li><li><a class="tocitem" href="../poisson/">1D Multi-scale Poisson&#39;s Equation</a></li><li><a class="tocitem" href="../convection/">1D Convection Equation</a></li><li><a class="tocitem" href="../helmholtz/">2D Helmholtz Equation</a></li><li><a class="tocitem" href="../allen_cahn/">Allen-Cahn Equation with Sequential Training</a></li><li><a class="tocitem" href="../SchrödingerEquation/">Schrödinger Equation: A PDE System with Resampling</a></li><li><a class="tocitem" href="../L_shape/">Poisson equation over an L-shaped domain</a></li><li><a class="tocitem" href="../waveinverse2/">Inverse problem for the wave equation with unknown velocity field</a></li><li><a class="tocitem" href="../sod/">SOD Shock Tube Problem</a></li></ul></li><li><a class="tocitem" href="../../qa/">FAQ</a></li><li><a class="tocitem" href="../../api/">API Reference</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Fitting a nonlinear discontinuous function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Fitting a nonlinear discontinuous function</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/YichengDWu/Sophon.jl/blob/main/docs/src/tutorials/discontinuous.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Fitting-a-nonlinear-discontinuous-function"><a class="docs-heading-anchor" href="#Fitting-a-nonlinear-discontinuous-function">Fitting a nonlinear discontinuous function</a><a id="Fitting-a-nonlinear-discontinuous-function-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-a-nonlinear-discontinuous-function" title="Permalink"></a></h1><p>This example is taken from <a href="https://royalsocietypublishing.org/doi/epdf/10.1098/rspa.2020.0334">here</a>. However, we do not use adaptive activation functions. Instead, we show that using suitable non-parametric activation functions immediately performs better.</p><p>Consider the following  discontinuous  function  with  discontinuity  at <span>$x=0$</span>:</p><p class="math-container">\[u(x)= \begin{cases}0.2 \sin (18 x) &amp; \text { if } x \leq 0 \\ 1+0.3 x \cos (54 x) &amp; \text { otherwise }\end{cases}\]</p><p>The domain is <span>$[-1,1]$</span>. The number of training points used is <code>50</code>.</p><h2 id="Import-packages"><a class="docs-heading-anchor" href="#Import-packages">Import packages</a><a id="Import-packages-1"></a><a class="docs-heading-anchor-permalink" href="#Import-packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using Lux, Sophon
using NNlib, Optimisers, Plots, Random, StatsBase, Zygote</code></pre><h2 id="Dataset"><a class="docs-heading-anchor" href="#Dataset">Dataset</a><a id="Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Dataset" title="Permalink"></a></h2><pre><code class="language-julia hljs">function u(x)
    if x &lt;= 0
        return 0.2 * sin(18 * x)
    else
        return 1 + 0.3 * x * cos(54 * x)
    end
end

function generate_data(n=50)
    x = reshape(collect(range(-1.0f0, 1.0f0, n)), (1, n))
    y = u.(x)
    return (x, y)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">generate_data (generic function with 2 methods)</code></pre><p>Let&#39;s visualize the data.</p><pre><code class="language-julia hljs">x_train, y_train = generate_data(50)
x_test, y_test = generate_data(200)
Plots.plot(vec(x_test), vec(y_test),label=false)</code></pre><p><img src="../u.svg" alt/></p><h2 id="Naive-Neural-Networks"><a class="docs-heading-anchor" href="#Naive-Neural-Networks">Naive Neural Networks</a><a id="Naive-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Naive-Neural-Networks" title="Permalink"></a></h2><p>First, we demonstrate that naive, fully connected neural nets are not sufficient for fitting this function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), relu)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, relu),     <span class="sgr90"># 100 parameters</span>
    layer_2 = Dense(50 =&gt; 50, relu),    <span class="sgr90"># 2_550 parameters</span>
    layer_3 = Dense(50 =&gt; 50, relu),    <span class="sgr90"># 2_550 parameters</span>
    layer_4 = Dense(50 =&gt; 50, relu),    <span class="sgr90"># 2_550 parameters</span>
    layer_5 = Dense(50 =&gt; 1),           <span class="sgr90"># 51 parameters</span>
) <span class="sgr90">        # Total: </span>7_801 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><h3 id="Train-the-model"><a class="docs-heading-anchor" href="#Train-the-model">Train the model</a><a id="Train-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-model" title="Permalink"></a></h3><pre><code class="language-julia hljs">function train(model, x, y)
    ps, st = Lux.setup(Random.default_rng(), model)
    opt = Adam()
    st_opt = Optimisers.setup(opt,ps)
    function loss(model, ps, st, x, y)
        y_pred, _ = model(x, ps, st)
        mes = mean(abs2, y_pred .- y)
        return mes
    end

    for i in 1:2000
        gs = gradient(p-&gt;loss(model,p,st,x,y), ps)[1]
        st_opt, ps = Optimisers.update(st_opt, ps, gs)
        if i % 100 == 1 || i == 2000
            println(&quot;Epoch $i ||  &quot;, loss(model,ps,st,x,y))
        end
    end
    return ps, st
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">train (generic function with 1 method)</code></pre><h3 id="Plot-the-result"><a class="docs-heading-anchor" href="#Plot-the-result">Plot the result</a><a id="Plot-the-result-1"></a><a class="docs-heading-anchor-permalink" href="#Plot-the-result" title="Permalink"></a></h3><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  1.3528609341073323
Epoch 101 ||  0.018994753544978617
Epoch 201 ||  0.01600983675009807
Epoch 301 ||  0.01587477723060479
Epoch 401 ||  0.015769730475146765
Epoch 501 ||  0.015726769172810084
Epoch 601 ||  0.015637439684837984
Epoch 701 ||  0.015505235819323655
Epoch 801 ||  0.015283792456134733
Epoch 901 ||  0.01490824608760921
Epoch 1001 ||  0.014325352186988756
Epoch 1101 ||  0.013717453987223153
Epoch 1201 ||  0.013314726735062139
Epoch 1301 ||  0.013018915198883919
Epoch 1401 ||  0.013008918117119577
Epoch 1501 ||  0.014340283806756277
Epoch 1601 ||  0.012599905822059866
Epoch 1701 ||  0.012539104631550928
Epoch 1801 ||  0.012497631933041242
Epoch 1901 ||  0.012500123492836183
Epoch 2000 ||  0.013564257053502043
  8.305783 seconds (9.42 M allocations: 1.228 GiB, 2.12% gc time, 94.38% compilation time)</code></pre><p><img src="../result1.svg" alt/></p><h2 id="Siren"><a class="docs-heading-anchor" href="#Siren">Siren</a><a id="Siren-1"></a><a class="docs-heading-anchor-permalink" href="#Siren" title="Permalink"></a></h2><p>We use four hidden layers with 50 neurons in each.</p><pre><code class="language-julia hljs">model = Siren(1,50,50,50,50,1; omega = 30f0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, sin),      <span class="sgr90"># 100 parameters</span>
    layer_2 = Dense(50 =&gt; 50, sin),     <span class="sgr90"># 2_550 parameters</span>
    layer_3 = Dense(50 =&gt; 50, sin),     <span class="sgr90"># 2_550 parameters</span>
    layer_4 = Dense(50 =&gt; 50, sin),     <span class="sgr90"># 2_550 parameters</span>
    layer_5 = Dense(50 =&gt; 1),           <span class="sgr90"># 51 parameters</span>
) <span class="sgr90">        # Total: </span>7_801 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.8170271475943891
Epoch 101 ||  0.0007284175094619472
Epoch 201 ||  1.2178284022784483e-5
Epoch 301 ||  1.6900688276964026e-7
Epoch 401 ||  2.1103216831508735e-9
Epoch 501 ||  1.5817878135412558e-11
Epoch 601 ||  3.913336118971192e-13
Epoch 701 ||  7.358507001052676e-14
Epoch 801 ||  6.903114750661731e-14
Epoch 901 ||  5.16569277264015e-14
Epoch 1001 ||  4.473669402169892e-14
Epoch 1101 ||  5.4909100136460505e-14
Epoch 1201 ||  2.911734502227864e-14
Epoch 1301 ||  3.710881315359265e-14
Epoch 1401 ||  4.2434767801850114e-14
Epoch 1501 ||  4.27165034172513e-14
Epoch 1601 ||  6.255366700128565e-14
Epoch 1701 ||  9.239075133398656e-14
Epoch 1801 ||  6.898255889642284e-14
Epoch 1901 ||  7.504438581275374e-14
Epoch 2000 ||  6.96971988028192e-14
  5.746772 seconds (5.28 M allocations: 1.090 GiB, 1.61% gc time, 90.38% compilation time)</code></pre><p><img src="../result.svg" alt/></p><p>As we can see the model overfits the data, and the high frequencies cannot be optimized away. We need to tunning the hyperparameter <code>omega</code></p><pre><code class="language-julia hljs">model = Siren(1,50,50,50,50,1; omega = 10f0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, sin),      <span class="sgr90"># 100 parameters</span>
    layer_2 = Dense(50 =&gt; 50, sin),     <span class="sgr90"># 2_550 parameters</span>
    layer_3 = Dense(50 =&gt; 50, sin),     <span class="sgr90"># 2_550 parameters</span>
    layer_4 = Dense(50 =&gt; 50, sin),     <span class="sgr90"># 2_550 parameters</span>
    layer_5 = Dense(50 =&gt; 1),           <span class="sgr90"># 51 parameters</span>
) <span class="sgr90">        # Total: </span>7_801 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.3065565608264555
Epoch 101 ||  0.0061154159184351655
Epoch 201 ||  0.003530250714655485
Epoch 301 ||  0.0022344290390485366
Epoch 401 ||  0.0015777923676829433
Epoch 501 ||  0.0009267852810835312
Epoch 601 ||  0.0005094944525018189
Epoch 701 ||  0.00032277848465224023
Epoch 801 ||  0.0002299234885339077
Epoch 901 ||  0.00016854942834649417
Epoch 1001 ||  0.00012329325940208178
Epoch 1101 ||  8.955526520631987e-5
Epoch 1201 ||  6.4970195993205e-5
Epoch 1301 ||  4.750991412575332e-5
Epoch 1401 ||  3.524973603382747e-5
Epoch 1501 ||  2.82281598182022e-5
Epoch 1601 ||  2.11987959550787e-5
Epoch 1701 ||  2.2447423520728264e-5
Epoch 1801 ||  1.4379651279569738e-5
Epoch 1901 ||  0.00028279911097905486
Epoch 2000 ||  1.0676302900266912e-5
  0.510637 seconds (1.26 M allocations: 901.899 MiB, 3.77% gc time)</code></pre><p><img src="../result10.svg" alt/></p><h2 id="Gaussian-activation-function"><a class="docs-heading-anchor" href="#Gaussian-activation-function">Gaussian activation function</a><a id="Gaussian-activation-function-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-activation-function" title="Permalink"></a></h2><p>We can also try using a fully connected net with the <a href="../../api/#Sophon.gaussian"><code>gaussian</code></a> activation function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), gaussian)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, gaussian),  <span class="sgr90"># 100 parameters</span>
    layer_2 = Dense(50 =&gt; 50, gaussian),  <span class="sgr90"># 2_550 parameters</span>
    layer_3 = Dense(50 =&gt; 50, gaussian),  <span class="sgr90"># 2_550 parameters</span>
    layer_4 = Dense(50 =&gt; 50, gaussian),  <span class="sgr90"># 2_550 parameters</span>
    layer_5 = Dense(50 =&gt; 1),           <span class="sgr90"># 51 parameters</span>
) <span class="sgr90">        # Total: </span>7_801 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.28481027616842025
Epoch 101 ||  0.003660321597836387
Epoch 201 ||  0.0009610560704229421
Epoch 301 ||  6.079970773925032e-5
Epoch 401 ||  4.962386973241526e-6
Epoch 501 ||  1.3723009070782544e-6
Epoch 601 ||  4.365766297540894e-7
Epoch 701 ||  5.10957930514898e-7
Epoch 801 ||  9.06280085065796e-8
Epoch 901 ||  0.004567726009175991
Epoch 1001 ||  7.714537861461366e-8
Epoch 1101 ||  7.486230434786125e-9
Epoch 1201 ||  1.596019261056928e-9
Epoch 1301 ||  2.868905809117079e-6
Epoch 1401 ||  2.2494974521682343e-5
Epoch 1501 ||  5.4020771193452895e-9
Epoch 1601 ||  8.0124766015634e-10
Epoch 1701 ||  1.064147153110827e-6
Epoch 1801 ||  3.612323547341919e-8
Epoch 1901 ||  1.6851912857542672e-9
Epoch 2000 ||  6.379976688441432e-5
  4.430731 seconds (4.76 M allocations: 1.060 GiB, 1.96% gc time, 85.59% compilation time)</code></pre><p><img src="../result2.svg" alt/></p><h2 id="Quadratic-activation-function"><a class="docs-heading-anchor" href="#Quadratic-activation-function">Quadratic activation function</a><a id="Quadratic-activation-function-1"></a><a class="docs-heading-anchor-permalink" href="#Quadratic-activation-function" title="Permalink"></a></h2><p><code>quadratic</code> is much cheaper to compute compared to the Gaussian activation function.</p><pre><code class="language-julia hljs">model = FullyConnected((1,50,50,50,50,1), quadratic)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 50, quadratic),  <span class="sgr90"># 100 parameters</span>
    layer_2 = Dense(50 =&gt; 50, quadratic),  <span class="sgr90"># 2_550 parameters</span>
    layer_3 = Dense(50 =&gt; 50, quadratic),  <span class="sgr90"># 2_550 parameters</span>
    layer_4 = Dense(50 =&gt; 50, quadratic),  <span class="sgr90"># 2_550 parameters</span>
    layer_5 = Dense(50 =&gt; 1),           <span class="sgr90"># 51 parameters</span>
) <span class="sgr90">        # Total: </span>7_801 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><pre><code class="language-julia hljs">@time ps, st = train(model, x_train, y_train)
y_pred = model(x_test,ps,st)[1]
Plots.plot(vec(x_test), vec(y_pred),label=&quot;Prediction&quot;,line = (:dot, 4))
Plots.plot!(vec(x_test), vec(y_test),label=&quot;Exact&quot;,legend=:topleft)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Epoch 1 ||  0.26593042226690145
Epoch 101 ||  0.00630371775442478
Epoch 201 ||  0.005568673821853717
Epoch 301 ||  0.004536874262227061
Epoch 401 ||  0.001948195929663213
Epoch 501 ||  0.00029608197458976177
Epoch 601 ||  1.887393318233747e-5
Epoch 701 ||  2.4592070351882225e-6
Epoch 801 ||  3.1126775576966457e-6
Epoch 901 ||  4.613613979119885e-8
Epoch 1001 ||  3.5637199003232445e-9
Epoch 1101 ||  0.0004070953161406657
Epoch 1201 ||  9.171495305978156e-8
Epoch 1301 ||  9.402156685360758e-7
Epoch 1401 ||  9.89236830701728e-10
Epoch 1501 ||  0.000550041952332479
Epoch 1601 ||  3.744373536015209e-8
Epoch 1701 ||  3.2067955988052154e-10
Epoch 1801 ||  4.491025296477609e-6
Epoch 1901 ||  3.016796354809872e-9
Epoch 2000 ||  1.209179518447441e-9
  4.365790 seconds (4.49 M allocations: 1.043 GiB, 2.02% gc time, 89.72% compilation time)</code></pre><p><img src="../result3.svg" alt/></p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>&quot;Neural networks suppress high-frequency components&quot; is a misinterpretation of the spectral bias. The accurate way of putting it is that the lower frequencies in the error are optimized first in the optimization process. This can be seen in Siren&#39;s example of overfitting data, where you do not have implicit regularization. The high frequency in the network will never go away because it has fitted the data perfectly.</p><p>Mainstream attributes the phenomenon that neural networks &quot;suppress&quot; high frequencies to gradient descent. This is not the whole picture. Initialization also plays an important role. Siren mitigates this problem by initializing larger weights in the first layer. In contrast, activation functions such as Gaussian have sufficiently large gradients and sufficiently large support of the second derivative with proper hyperparameters. Please refer to [<a href="../../references/#sitzmann2020implicit">1</a>], [<a href="../../references/#ramasinghe2021beyond">2</a>] and [<a href="../../references/#ramasinghe2022regularizing">3</a>] if you want to dive deeper into this.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ode/">« Introduction with Lotka-Volterra System</a><a class="docs-footer-nextpage" href="../poisson/">1D Multi-scale Poisson&#39;s Equation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Saturday 6 January 2024 09:04">Saturday 6 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
