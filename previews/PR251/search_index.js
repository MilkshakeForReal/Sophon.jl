var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"V. Sitzmann, J. Martel, A. Bergman, D. Lindell and G. Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems 33, 7462–7473 (2020).\n\n\n\nS. Ramasinghe and S. Lucey. Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps, arXiv preprint arXiv:2111.15135 (2021).\n\n\n\nS. Ramasinghe, L. MacDonald and S. Lucey. On Regularizing Coordinate-MLPs, arXiv preprint arXiv:2202.00790 (2022).\n\n\n\nS. Wang, H. Wang and P. Perdikaris. On the eigenvector bias of fourier feature networks: From regression to solving multi-scale pdes with physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering 384, 113938 (2021).\n\n\n\nL. Lu, P. Jin, G. Pang, Z. Zhang and G. E. Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence 3, 218–229 (2021).\n\n\n\nD. B. Lindell, D. Van Veen, J. J. Park and G. Wetzstein. BACON: Band-limited coordinate networks for multiscale scene representation, arXiv preprint arXiv:0000.00000 (2021).\n\n\n\nS. Wang, H. Wang, J. H. Seidman and P. Perdikaris. Random Weight Factorization Improves the Training of Continuous Neural Representations, arXiv preprint arXiv:2210.01274 (2022).\n\n\n\nA. Rahimi and B. Recht. Random features for large-scale kernel machines. Advances in neural information processing systems 20 (2007).\n\n\n\nM. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems 33, 7537–7547 (2020).\n\n\n\nS. Wang, Y. Teng and P. Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing 43, A3055–A3081 (2021).\n\n\n\nR. Fathony, A. K. Sahu, D. Willmott and J. Z. Kolter. Multiplicative Filter Networks. In: International Conference on Learning Representations (2021).\n\n\n\nR. Gnanasambandam, B. Shen, J. Chung, X. Yue and others. Self-scalable tanh (stan): Faster convergence and better generalization in physics-informed neural networks, arXiv preprint arXiv:2204.12589 (2022).\n\n\n\n","category":"page"},{"location":"tutorials/allen_cahn/#Allen-Cahn-Equation-with-Sequential-Training","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"","category":"section"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"In this tutorial we are going to solve the Allen-Cahn equation with periodic boundary condition from t=0 to t=1. The traning process is split into four stages, namely  tin 0025, tin 0005, tin 00075 and tin 00 10.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"using ModelingToolkit, IntervalSets\nusing Sophon\nusing Optimization, OptimizationOptimJL, Zygote\n\n@parameters t, x\n@variables u(..)\nDₓ = Differential(x)\nDₓ² = Differential(x)^2\nDₜ = Differential(t)\n\neq = Dₜ(u(x, t)) - 0.0001 * Dₓ²(u(x, t)) + 5 * u(x,t) * (abs2(u(x,t)) - 1.0) ~ 0.0\n\ndomain = [x ∈ -1.0..1.0, t ∈ 0.0..0.25]\n\nbcs = [u(x,0) ~ x^2 * cospi(x),\n       u(-1,t) ~ u(1,t)]\n\n@named allen = PDESystem(eq, bcs, domain, [x, t], [u(x, t)])","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"Then we define the neural net, the sampler, and the training strategy.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"chain = FullyConnected(2, 1, tanh; hidden_dims=16, num_layers=4)\npinn = PINN(chain)\nsampler = QuasiRandomSampler(500, (300, 100))\nstrategy = NonAdaptiveTraining(1, (50, 1))\nprob = Sophon.discretize(allen, pinn, sampler, strategy)","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"We solve the equation sequentially in time.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"function train(allen, prob, sampler, strategy)\n    bfgs = BFGS()\n    res = Optimization.solve(prob, bfgs; maxiters=2000)\n\n    for tmax in [0.5, 0.75, 1.0]\n        allen.domain[2] = t ∈ 0.0..tmax\n        data = Sophon.sample(allen, sampler)\n        prob = remake(prob; u0=res.u, p=data)\n        res = Optimization.solve(prob, bfgs; maxiters=2000)\n    end\n    return res\nend\n\nres = train(allen, prob, sampler, strategy)","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"Let's plot the result.","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"using CairoMakie\n\nphi = pinn.phi\nxs, ts = [infimum(d.domain):0.01:supremum(d.domain) for d in allen.domain]\naxis = (xlabel=\"t\", ylabel=\"x\", title=\"Prediction\")\nu_pred = [sum(pinn.phi([x, t], res.u)) for x in xs, t in ts]\nfig, ax, hm = heatmap(ts, xs, u_pred', axis=axis)\n\nsave(\"allen.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/allen_cahn/","page":"Allen-Cahn Equation with Sequential Training","title":"Allen-Cahn Equation with Sequential Training","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/#Schrödinger-equation","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger equation","text":"","category":"section"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"The nonlinear Shrödinger equation is given by","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"mathrmi partial_t psi=-frac12 sigma partial_x x psi-betapsi^2 psi","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"Let sigma=beta=1 psi=u+v i, the equation can be transformed into a system of partial differential equations","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"using ModelingToolkit, IntervalSets, Sophon, CairoMakie\nusing Optimization, OptimizationOptimJL, Zygote\n\n@parameters x,t\n@variables u(..), v(..)\nDₜ = Differential(t)\nDₓ² = Differential(x)^2\n\neqs=[Dₜ(u(x,t)) ~ -Dₓ²(v(x,t))/2 - (abs2(v(x,t)) + abs2(u(x,t))) * v(x,t),\n     Dₜ(v(x,t)) ~  Dₓ²(u(x,t))/2 + (abs2(v(x,t)) + abs2(u(x,t))) * u(x,t)]\n\nbcs = [u(x, 0.0) ~ 2sech(x),\n       v(x, 0.0) ~ 0.0,\n       u(-5.0, t) ~ u(5.0, t),\n       v(-5.0, t) ~ v(5.0, t)]\n\ndomains = [x ∈ Interval(-5.0, 5.0),\n           t ∈ Interval(0.0, π/2)]\n\n@named pde_system = PDESystem(eqs, bcs, domains, [x,t], [u(x,t),v(x,t)])","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"pinn = PINN(u = Siren(2,1; hidden_dims=16,num_layers=4, omega = 1.0),\n            v = Siren(2,1; hidden_dims=16,num_layers=4, omega = 1.0))\n            \nsampler = QuasiRandomSampler(500, (200,200,20,20))\nstrategy = NonAdaptiveTraining(1,(10,10,1,1))\n\nprob = Sophon.discretize(pde_system, pinn, sampler, strategy)","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"Now we train the neural nets and resample data while training.","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"function train(pde_system, prob, sampler, strategy, resample_period = 500, n=10)\n     bfgs = BFGS()\n     res = Optimization.solve(prob, bfgs; maxiters=2000)\n     \n     for i in 1:n\n         data = Sophon.sample(pde_system, sampler)\n         prob = remake(prob; u0=res.u, p=data)\n         @showprogress res = Optimization.solve(prob, bfgs; maxiters=resample_period)\n     end\n     return res\nend\n\nres = train(pde_system, prob, sampler, strategy)","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"phi = pinn.phi\nps = res.u\n\nxs, ts= [infimum(d.domain):0.01:supremum(d.domain) for d in pde_system.domain]\n\nu = [sum(phi.u(([x,t]), ps.u)) for x in xs, t in ts]\nv = [sum(phi.v(([x,t]), ps.v)) for x in xs, t in ts]\nψ = @. sqrt(u^2+ v^2)\n\naxis = (xlabel=\"t\", ylabel=\"x\", title=\"u\")\nfig, ax1, hm1 = heatmap(ts, xs, u', axis=axis)\nax2, hm2= heatmap(fig[1, end+1], ts, xs, v', axis= merge(axis, (; title=\"v\")))\ndisplay(fig)\nsave(\"uv.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"axis = (xlabel=\"t\", ylabel=\"x\", title=\"ψ\")\nfig, ax1, hm1 = heatmap(ts, xs, ψ', axis=axis, colormap=:jet)\nColorbar(fig[:, end+1], hm1)\ndisplay(fig)\nsave(\"phi.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/#Customize-Sampling","page":"Schrödinger Equation: A PDE System with Resampling","title":"Customize Sampling","text":"","category":"section"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"Bascially any sampling method is supportted. For example we can sample data according to the predicted solution.","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"using StatsBase\n\ndata = vec([[x, t] for x in xs, t in ts])\nwv = vec(ψ)\nnew_data = wsample(data, wv, 500)\nnew_data = reduce(hcat, new_data)\nfig, ax = scatter(new_data[2,:], new_data[1,:])\nsave(\"data.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"(Image: )","category":"page"},{"location":"tutorials/SchrödingerEquation/","page":"Schrödinger Equation: A PDE System with Resampling","title":"Schrödinger Equation: A PDE System with Resampling","text":"prob.p[1] = new_data\nprob.p[2] = new_data\nprob = remake(prob; u0 = res.u)\n# res = Optimization.solve(prob, bfgs; maxiters=1000)","category":"page"},{"location":"tutorials/inverse/#Inverse-Problem-of-the-Lorenz-System","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"","category":"section"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"fracmathrmd xmathrmd t=sigma(y-x)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":",","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"fracmathrmd ymathrmd t=x(rho-z)-y","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":",","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"fracmathrmd zmathrmd t=x y-beta z","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":",","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"using ModelingToolkit, Sophon, OrdinaryDiffEq\nusing Optimization, OptimizationOptimJL, Zygote\nusing ModelingToolkit, IntervalSets","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"@parameters t \n@variables x(..), y(..), z(..), σ(..), β(..), ρ(..)\n\nDt = Differential(t)\neqs = [Dt(x(t)) ~ σ(t)*(y(t) - x(t)),\n       Dt(y(t)) ~ x(t)*(ρ(t) - z(t)) - y(t),\n       Dt(z(t)) ~ x(t)*y(t) - β(t)*z(t)]\n\nbcs = [x(0) ~ 1.0, y(0) ~ 0.0, z(0) ~ 0.0]\ndomains = [t ∈ Interval(0.0,1.0)]\n@named pde_system = PDESystem(eqs, bcs, domains, [t], [x(t),y(t),z(t),σ(t), ρ(t), β(t)])","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"function lorenz!(du,u,p,t)\n    du[1] = 10.0*(u[2]-u[1])\n    du[2] = u[1]*(28.0-u[3]) - u[2]\n    du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,1.0)\nprob = ODEProblem(lorenz!,u0,tspan)\nsol = solve(prob, Tsit5(), dt=0.1)\nts = [infimum(d.domain):0.1:supremum(d.domain) for d in domains][1]\nfunction getData(sol)\n    data = []\n    us = hcat(sol(ts).u...)\n    ts_ = hcat(sol(ts).t...)\n    return [us,ts_]\nend\ndata = getData(sol)\n\n(u_ , t_) = data","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"pinn = PINN(x = FullyConnected((1,16,16,16,1), tanh),\n            y = FullyConnected((1,16,16,16,1), tanh),\n            z = FullyConnected((1,16,16,16,1), tanh),\n            σ = ConstantFunction(),\n            ρ = ConstantFunction(),\n            β = ConstantFunction())\nsampler = QuasiRandomSampler(100, 1)\nstrategy = NonAdaptiveTraining()\n\nt_data = t_\nu_data = u_ \nfunction additional_loss(phi, θ)\n    return sum(abs2, vcat(phi.x(t_data, θ.x), phi.y(t_data, θ.y), phi.z(t_data, θ.z)).-u_data)/length(t_data)\nend\nprob = Sophon.discretize(pde_system, pinn, sampler, strategy, additional_loss=additional_loss)\n\n@showprogress res = Optimization.solve(prob, BFGS(), maxiters=1000)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"print(res.u.σ.constant, res.u.ρ.constant, res.u.β.constant)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"phi=pinn.phi\nθ = res.u\nts=  [0:0.01:1...;;]\nx_pred = phi.x(ts, θ.x)\ny_pred = phi.x(ts, θ.y)\nz_pred = phi.x(ts, θ.z)","category":"page"},{"location":"tutorials/inverse/","page":"Inverse Problem of the Lorenz System","title":"Inverse Problem of the Lorenz System","text":"using Plots\nPlots.plot(vec(ts), [vec(x_pred),vec(y_pred),vec(z_pred)],  label=[\"x(t)\" \"y(t)\" \"z(t)\"])   ","category":"page"},{"location":"tutorials/ode/#Getting-started-with-ODEs","page":"Introduction with Lotka-Volterra System","title":"Getting started with ODEs","text":"","category":"section"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"This tutorial provides a step-by-step guide to solve the Lotka-Volterra system of ordinary differential equations (ODEs).","category":"page"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"using ModelingToolkit\nusing Sophon, IntervalSets\nusing Optimization, OptimizationOptimJL, Zygote\nusing Plots\n\n# Defining parameters and variables\n@parameters t\n@variables x(..), y(..)\n\n# Define the differential operator\nDₜ = Differential(t)\np = [1.5, 1.0, 3.0, 1.0]\n\n# Setting up the system of equations\neqs = [Dₜ(x(t)) ~ p[1] * x(t) - p[2] * x(t) * y(t),\n      Dₜ(y(t)) ~ -p[3] * y(t) + p[4] * x(t) * y(t)]\n\n# Defining the domain\ndomain = [t ∈ 0 .. 3.0]\n\n# Defining the initial conditions\nbcs = [x(0.0) ~ 1.0, y(0.0) ~ 1.0]\n\n@named lotka_volterra = PDESystem(eqs, bcs, domain, [t], [x(t), y(t)])","category":"page"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"In this part of the tutorial, we will employ BetaRandomSampler to generate training data using a Beta distribution. This introduces a soft causality into the training data, enhancing the effectiveness of the learning process.","category":"page"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"# Constructing the physics-informed neural network (PINN)\npinn = PINN(x = FullyConnected(1, 1, sin; hidden_dims=8, num_layers=3),\n            y = FullyConnected(1, 1, sin; hidden_dims=8, num_layers=3))\n\n# Setting up the sampler, training strategy and problem\nsampler = BetaRandomSampler(200, 1)\nstrategy = NonAdaptiveTraining(1,50)\nprob = Sophon.discretize(lotka_volterra, pinn, sampler, strategy)\n\n# Solving the problem using BFGS optimization\n@showprogress res = Optimization.solve(prob, BFGS(); maxiters=1000)","category":"page"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"Next, we'll compare our results with a reference solution to verify our computations.","category":"page"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"using OrdinaryDiffEq\n\nfunction f(u, p, t)\n    return [p[1] * u[1] - p[2] * u[1] * u[2], -p[3] * u[2] + p[4] * u[1] * u[2]]\nend\n\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0, 1.0]\nprob_oop = ODEProblem{false}(f, u0, (0.0, 3.0), p)\ntrue_sol = solve(prob_oop, Tsit5(), saveat=0.01)\n\nphi = pinn.phi\nts = [true_sol.t...;;]\nx_pred = phi.x(ts, res.u.x)\ny_pred = phi.y(ts, res.u.y)\n\nplot(vec(ts), vec(x_pred), label=\"x_pred\")\nplot!(vec(ts), vec(y_pred), label=\"y_pred\")\nplot!(true_sol)","category":"page"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"While the initial results are encouraging, we can further refine our model. By remaking the sampler, we can gradually transition to the uniform distribution for improved results.","category":"page"},{"location":"tutorials/ode/","page":"Introduction with Lotka-Volterra System","title":"Introduction with Lotka-Volterra System","text":"# Adjusting the sampler to uniform distribution and re-solving the problem\nfor α in [0.6, 0.8, 1.0] # when α = 1.0, it is equivalent to uniform sampling\n    sampler = remake(sampler; α=α)\n    data = Sophon.sample(lotka_volterra, sampler)\n    prob = remake(prob; p=data, u0=res.u)\n    res = Optimization.solve(prob, BFGS(); maxiters=1000)\nend\n\n# Generating new predictions and calculating the absolute error\nx_pred = phi.x(ts, res.u.x)\ny_pred = phi.y(ts, res.u.y)\nmaximum(sum(abs2, vcat(x_pred, y_pred) .- stack(true_sol.u); dims=1)) # print the absolute error","category":"page"},{"location":"tutorials/discontinuous/#Fitting-a-nonlinear-discontinuous-function","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"This example is taken from here. However, we do not use adaptive activation functions. Instead, we show that using suitable non-parametric activation functions immediately performs better.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"Consider the following  discontinuous  function  with  discontinuity  at x=0:","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"u(x)= begincases02 sin (18 x)  text  if  x leq 0  1+03 x cos (54 x)  text  otherwise endcases","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"The domain is -11. The number of training points used is 50.","category":"page"},{"location":"tutorials/discontinuous/#Import-packages","page":"Fitting a nonlinear discontinuous function","title":"Import packages","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"using Lux, Sophon\nusing NNlib, Optimisers, Plots, Random, StatsBase, Zygote","category":"page"},{"location":"tutorials/discontinuous/#Dataset","page":"Fitting a nonlinear discontinuous function","title":"Dataset","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"function u(x)\n    if x <= 0\n        return 0.2 * sin(18 * x)\n    else\n        return 1 + 0.3 * x * cos(54 * x)\n    end\nend\n\nfunction generate_data(n=50)\n    x = reshape(collect(range(-1.0f0, 1.0f0, n)), (1, n))\n    y = u.(x)\n    return (x, y)\nend","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"Let's visualize the data.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"x_train, y_train = generate_data(50)\nx_test, y_test = generate_data(200)\nPlots.plot(vec(x_test), vec(y_test),label=false)\nsavefig(\"u.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Naive-Neural-Networks","page":"Fitting a nonlinear discontinuous function","title":"Naive Neural Networks","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"First, we demonstrate that naive, fully connected neural nets are not sufficient for fitting this function.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = FullyConnected((1,50,50,50,50,1), relu)","category":"page"},{"location":"tutorials/discontinuous/#Train-the-model","page":"Fitting a nonlinear discontinuous function","title":"Train the model","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"function train(model, x, y)\n    ps, st = Lux.setup(Random.default_rng(), model)\n    opt = Adam()\n    st_opt = Optimisers.setup(opt,ps)\n    function loss(model, ps, st, x, y)\n        y_pred, _ = model(x, ps, st)\n        mes = mean(abs2, y_pred .- y)\n        return mes\n    end\n\n    for i in 1:2000\n        gs = gradient(p->loss(model,p,st,x,y), ps)[1]\n        st_opt, ps = Optimisers.update(st_opt, ps, gs)\n        if i % 100 == 1 || i == 2000\n            println(\"Epoch $i ||  \", loss(model,ps,st,x,y))\n        end\n    end\n    return ps, st\nend\n","category":"page"},{"location":"tutorials/discontinuous/#Plot-the-result","page":"Fitting a nonlinear discontinuous function","title":"Plot the result","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result1.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Siren","page":"Fitting a nonlinear discontinuous function","title":"Siren","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"We use four hidden layers with 50 neurons in each.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = Siren(1,50,50,50,50,1; omega = 30f0)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"As we can see the model overfits the data, and the high frequencies cannot be optimized away. We need to tunning the hyperparameter omega","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = Siren(1,50,50,50,50,1; omega = 10f0)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result10.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Gaussian-activation-function","page":"Fitting a nonlinear discontinuous function","title":"Gaussian activation function","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"We can also try using a fully connected net with the gaussian activation function.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = FullyConnected((1,50,50,50,50,1), gaussian)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result2.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Quadratic-activation-function","page":"Fitting a nonlinear discontinuous function","title":"Quadratic activation function","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"quadratic is much cheaper to compute compared to the Gaussian activation function.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"model = FullyConnected((1,50,50,50,50,1), quadratic)","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"@time ps, st = train(model, x_train, y_train)\ny_pred = model(x_test,ps,st)[1]\nPlots.plot(vec(x_test), vec(y_pred),label=\"Prediction\",line = (:dot, 4))\nPlots.plot!(vec(x_test), vec(y_test),label=\"Exact\",legend=:topleft)\nsavefig(\"result3.svg\"); nothing # hide","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"(Image: )","category":"page"},{"location":"tutorials/discontinuous/#Conclusion","page":"Fitting a nonlinear discontinuous function","title":"Conclusion","text":"","category":"section"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"\"Neural networks suppress high-frequency components\" is a misinterpretation of the spectral bias. The accurate way of putting it is that the lower frequencies in the error are optimized first in the optimization process. This can be seen in Siren's example of overfitting data, where you do not have implicit regularization. The high frequency in the network will never go away because it has fitted the data perfectly.","category":"page"},{"location":"tutorials/discontinuous/","page":"Fitting a nonlinear discontinuous function","title":"Fitting a nonlinear discontinuous function","text":"Mainstream attributes the phenomenon that neural networks \"suppress\" high frequencies to gradient descent. This is not the whole picture. Initialization also plays an important role. Siren mitigates this problem by initializing larger weights in the first layer. In contrast, activation functions such as Gaussian have sufficiently large gradients and sufficiently large support of the second derivative with proper hyperparameters. Please refer to [1], [2] and [3] if you want to dive deeper into this.","category":"page"},{"location":"qa/#Q:-How-can-I-train-the-model-using-GPUs?","page":"FAQ","title":"Q: How can I train the model using GPUs?","text":"","category":"section"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"A: To train the model on GPUs, invoke the gpu function on instances of PINN:","category":"page"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"using Lux\npinn = gpu(PINN(...))","category":"page"},{"location":"qa/#Q:-How-can-I-monitor-the-loss-for-each-loss-function?","page":"FAQ","title":"Q: How can I monitor the loss for each loss function?","text":"","category":"section"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"A: Data points are stored in prob.p. Call Sophon.residual_function_x with the corresponding arguments to obtain the residual of each data point:","category":"page"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"residual = Sophon.residual_function_1(prob.p[1], res.u)","category":"page"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"If you want to monitor the loss during training, create a callback function like the following:","category":"page"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"function callback(p, _)\n    loss_1 = sum(abs2, Sophon.residual_function_1(prob.p[1], p))\n    loss_2 = sum(abs2, Sophon.residual_function_1(prob.p[2], p))\n    println(\"loss: $loss_1, loss_2\")\n    return false\nend","category":"page"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"Finally, pass the callback function to Optimization.solve to monitor the loss as the training progresses.","category":"page"},{"location":"qa/#Q:-How-can-I-inspect-the-generated-symbolic-loss-function?","page":"FAQ","title":"Q: How can I inspect the generated symbolic loss function?","text":"","category":"section"},{"location":"qa/","page":"FAQ","title":"FAQ","text":"A: Simply replace Sophon.discretize by Sophon.symbolic_discretize.","category":"page"},{"location":"tutorials/poisson/#D-Poisson's-Equation","page":"1D Multi-scale Poisson's Equation","title":"1D Poisson's Equation","text":"","category":"section"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"This example is taken from [4]. Consider a simple 1D Poisson’s equation with Dirichlet boundary conditions. The solution is given by","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"u(x)=sin (2 pi x)+01 sin (50 pi x)","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"using ModelingToolkit, IntervalSets, Sophon\nusing Optimization, OptimizationOptimJL, Zygote\nusing CairoMakie\n\n@parameters x\n@variables u(..)\nDₓ² = Differential(x)^2\n\nf(x) = -4 * π^2 * sin(2 * π * x) - 250 * π^2 * sin(50 * π * x)\neq = Dₓ²(u(x)) ~ f(x)\ndomain = [x ∈ 0 .. 1]\nbcs = [u(0) ~ 0, u(1) ~ 0]\n\n@named poisson = PDESystem(eq, bcs, domain, [x], [u(x)])","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"chain = Siren(1, 16, 32, 16, 1)\npinn = PINN(chain)\nsampler = QuasiRandomSampler(200, 1) \nstrategy = NonAdaptiveTraining(1 , 50)\n\nprob = Sophon.discretize(poisson, pinn, sampler, strategy)\n@showprogress res = Optimization.solve(prob, BFGS(); maxiters=2000)\n\nphi = pinn.phi\nxs = 0:0.001:1\nu_true = @. sin(2 * pi * xs) + 0.1 * sin(50 * pi * xs)\nus = phi(xs', res.u)\nfig = Figure()\naxis = Axis(fig[1, 1])\nlines!(xs, u_true; label=\"Ground Truth\")\nlines!(xs, vec(us); label=\"Prediction\")\naxislegend(axis)\nfig\nsave(\"result.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"(Image: )","category":"page"},{"location":"tutorials/poisson/#Compute-the-relative-L2-error","page":"1D Multi-scale Poisson's Equation","title":"Compute the relative L2 error","text":"","category":"section"},{"location":"tutorials/poisson/","page":"1D Multi-scale Poisson's Equation","title":"1D Multi-scale Poisson's Equation","text":"using Integrals\n\nu_analytical(x,p) = sin.(2 * pi .* x) + 0.1 * sin.(50 * pi .* x)\nerror(x,p) = abs2.(vec(phi([x;;],res.u)) .- u_analytical(x,p))\n\nrelative_L2_error = solve(IntegralProblem(error,0,1),HCubatureJL(),reltol=1e-3,abstol=1e-3) ./ solve(IntegralProblem((x,p) -> abs2.(u_analytical(x,p)),0, 1),HCubatureJL(),reltol=1e-3,abstol=1e-3)","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [Sophon]\nPrivate = false","category":"page"},{"location":"api/#Sophon.AdaptiveTraining","page":"API Reference","title":"Sophon.AdaptiveTraining","text":"AdaptiveTraining(pde_weights, bcs_weights)\n\nAdaptive weights for the loss functions. Here pde_weights and bcs_weights are functions that take in (phi, x, θ) and return the point-wise weights. Note that bcs_weights can be real numbers but they will be converted to functions that return the same numbers.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.BetaRandomSampler","page":"API Reference","title":"Sophon.BetaRandomSampler","text":"BetaRandomSampler(pde_points, bcs_points=pde_points; sampling_alg=SobolSample(),\n                  resample::Bool=false, α=0.4, β=1.0)\n\nSame as QuasiRandomSampler, but use Beta distribution along time on the domain.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.ChainState","page":"API Reference","title":"Sophon.ChainState","text":"ChainState(model, rng::AbstractRNG=Random.default_rng())\n\nIt this similar to Lux.Chain but wraps it in a stateful container.\n\nFields\n\nmodel: The neural network.\nstates: The states of the neural network.\n\nInput\n\nx: The input to the neural network.\nps: The parameters of the neural network.\n\nArguments\n\nmodel: AbstractExplicitLayer, or a named tuple of them, which will be treated as a Chain.\nrng: AbstractRNG to use for initialising the neural network.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.ConstantFunction","page":"API Reference","title":"Sophon.ConstantFunction","text":"ConstantFunction()\n\nA conatiner for scalar parameter. This is useful for the case that you want a dummy layer that returns the scalar parameter for any input.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.DeepONet","page":"API Reference","title":"Sophon.DeepONet","text":"DeepONet(branch_net, trunk_net;\n         flatten_layer=FlattenLayer(),\n         linear_layer=NoOpLayer(),\n         bias=ScalarLayer())\nDeepONet(layer_sizes_branch, activation_branch,\n         layer_sizes_trunk,\n         activation_trunk,\n         layer_sizes_linear=nothing)\n\nDeep operator network. Note that the branch net supports multi-dimensional inputs. The flatten_layer flatten the output of the branch net to a matrix, and the linear_layer is applied to the flattened. In this case, linear_layer must be given to transform the flattened matrix to the correct shape.\n\nv → branch_net → flatten_layer → linear_layer → b\n                                                  ↘\n                                                    b' * t + bias → u\n                                                  ↗\n                                ξ → trunk_net → t\n\nArguments\n\nbranch_net: The branch net.\ntrunk_net: The trunk net.\n\nKeyword Arguments\n\nflatten_layer: The layer to flatten a multi-dimensional array to a matrix.\nlinear_layer: The layer to apply a linear transformation to the output of the flatten_layer.\n\nInputs\n\n(v, ξ): v is an array of shape (b_1b_2b_d m), which is a discretization of m functions from R^d to R. ξ is a matrix of shape (d n), representing n data points of the domain R^d.\n\nReturns\n\nA matrix of shape (m n).\n\nExamples\n\njulia> deeponet = DeepONet((3, 5, 4), relu, (2, 6, 4, 4), tanh)\nDeepONet(\n    branch_net = Chain(\n        layer_1 = Dense(3 => 5, relu),  # 20 parameters\n        layer_2 = Dense(5 => 4),        # 24 parameters\n    ),\n    trunk_net = Chain(\n        layer_1 = Dense(2 => 6, tanh_fast),  # 18 parameters\n        layer_2 = Dense(6 => 4, tanh_fast),  # 28 parameters\n        layer_3 = Dense(4 => 4, tanh_fast),  # 20 parameters\n    ),\n    flatten_layer = FlattenLayer(),\n    linear_layer = NoOpLayer(),\n    bias = ScalarLayer(),                    # 1 parameters\n)         # Total: 111 parameters,\n          #        plus 0 states, summarysize 80 bytes.\n\nReference\n\n[5]\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.DiscreteFourierFeature","page":"API Reference","title":"Sophon.DiscreteFourierFeature","text":"DiscreteFourierFeature(in_dims::Int, out_dims::Int, N::Int, period::Real)\n\nThe discrete Fourier filter proposed in [6]. For a periodic function with period P, the Fourier series in amplitude-phase form is\n\ns_N(x)=fraca_02+sum_n=1^Na_ncdot sin left( frac2piPnx+varphi _n right)\n\nThe output is guaranteed to be periodic.\n\nArguments\n\nin_dims: Number of the input dimensions.\nout_dims: Number of the output dimensions.\nN: N in the formula.\nperiod: P in the formula.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.FactorizedDense","page":"API Reference","title":"Sophon.FactorizedDense","text":"FactorizedDense(in_dims::Int, out_dims::Int, activation=identity;\n                     mean::AbstractFloat=1.0f0, std::AbstractFloat=0.1f0,\n                     init_weight=kaiming_uniform(activation), init_bias=zeros32)\n\nCreate a Dense layer where the weight is factorized into twa parts, the scaling factors for each row and the weight matrix.\n\nArguments\n\nin_dims: number of input dimensions\nout_dims: number of output dimensions\nactivation: activation function\n\nKeyword Arguments\n\nmean: mean of the scaling factors\nstd: standard deviation of the scaling factors\ninit_weight: weight initialization function\ninit_bias: bias initialization function\n\nInput\n\nx: input vector or matrix\n\nReturns\n\ny = activation.(scale * weight * x+ bias).\nEmpty NamedTuple().\n\nParameters\n\nscale: scaling factors. Shape: (out_dims, 1)\nweight: Weight Matrix of size (out_dims, in_dims).\nbias: Bias of size (out_dims, 1).\n\nReferences\n\n[7]\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.FourierFeature","page":"API Reference","title":"Sophon.FourierFeature","text":"FourierFeature(in_dims::Int, std::NTuple{N,Pair{S,T}}) where {N,S,T<:Int}\nFourierFeature(in_dims::Int, frequencies::NTuple{N, T}) where {N, T <: Real}\nFourierFeature(in_dims::Int, out_dims::Int, std::Real)\n\nFourier Feature Network.\n\nArguments\n\nin_dims: Number of the input dimensions.\nstd: A tuple of pairs of sigma => out_dims, where sigma is the standard deviation of the Gaussian distribution.\n\nphi^(i)(x)=leftsin left(2 pi W^(i) xright)  cos 2 pi W^(i) xright W^(i) sim mathcalNleft(0 sigma^(i)right) iin 1 dots D\n\nfrequencies: A tuple of frequencies (f1f2fn).\n\nphi^(i)(x)=leftsin left(2 pi f_i xright)  cos 2 pi f_i xright\n\nParameters\n\nIf std is used, then parameters are Ws in the formula.\n\nInputs\n\nx: AbstractArray with size(x, 1) == in_dims.\n\nReturns\n\nphi^(1) phi^(2)  phi^(D) with size(y, 1) == sum(last(modes) * 2).\n\nExamples\n\njulia> f = FourierFeature(2,10,1) # Random Fourier Feature\nFourierFeature(2 => 10)\n\njulia> f = FourierFeature(2, (1 => 3, 50 => 4)) # Multi-scale Random Fourier Features\nFourierFeature(2 => 14)\n\njulia>  f = FourierFeature(2, (1,2,3,4)) # Predefined frequencies\nFourierFeature(2 => 16)\n\nReferences\n\n[8]\n\n[9]\n\n[4]\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.NonAdaptiveTraining","page":"API Reference","title":"Sophon.NonAdaptiveTraining","text":"NonAdaptiveTraining(pde_weights=1, bcs_weights=pde_weights)\n\nFixed weights for the loss functions.\n\nArguments\n\npde_weights: weights for the PDE loss functions. If a single number is given, it is used for all PDE loss functions.\nbcs_weights: weights for the boundary conditions loss functions. If a single number is given, it is used for all boundary conditions loss functions.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.PINN","page":"API Reference","title":"Sophon.PINN","text":"PINN(chain, rng::AbstractRNG=Random.default_rng())\nPINN(rng::AbstractRNG=Random.default_rng(); kwargs...)\n\nA container for a neural network, its states and its initial parameters. Call gpu and cpu to move the neural network to the GPU and CPU respectively. The default element type of the parameters is Float64.\n\nFields\n\nphi: ChainState if there is only one neural network, or an named tuple of ChainStates if there are multiple neural networks. The names are the same as the dependent variables in the PDE.\ninit_params: The initial parameters of the neural network.\n\nArguments\n\nchain: AbstractExplicitLayer or a named tuple of AbstractExplicitLayers.\nrng: AbstractRNG to use for initialising the neural network. If yout want to set the seed, write\n\nusing Random\nrng = Random.default_rng()\nRandom.seed!(rng, 0)d\n\nand pass rng to PINN as\n\nusing Sophon\n\nchain = FullyConnected((1, 6, 6, 1), sin);\n\n# sinple dependent varibale\npinn = PINN(chain, rng);\n\n# multiple dependent varibales\npinn = PINN(rng; a=chain, b=chain);\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.PINNAttention","page":"API Reference","title":"Sophon.PINNAttention","text":"PINNAttention(H_net, U_net, V_net, fusion_layers)\nPINNAttention(in_dims::Int, out_dims::Int, activation::Function=sin;\n              hidden_dims::Int, num_layers::Int)\n\nThe output dimesion of H_net and the input dimension of fusion_layers must be the same. For the second and the third constructor, Dense layers is used for H_net, U_net, and V_net. Note that the first constructer does not contain the output layer, but the second one does.\n\n                 x → U_net → u                           u\n                               ↘                           ↘\nx → H_net →  h1 → fusionlayer1 → connection → fusionlayer2 → connection\n                               ↗                           ↗\n                 x → V_net → v                           v\n\nArguments\n\nH_net: AbstractExplicitLayer.\nU_net: AbstractExplicitLayer.\nV_net: AbstractExplicitLayer.\nfusion_layers: Chain.\n\nKeyword Arguments\n\nnum_layers: The number of hidden layers.\nhidden_dims: The number of hidden dimensions of each hidden layer.\n\nReference\n\n[10]\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.QuasiRandomSampler","page":"API Reference","title":"Sophon.QuasiRandomSampler","text":"QuasiRandomSampler(pde_points, bcs_points=pde_points;\n                   sampling_alg=SobolSample(),\n                   resample = false))\n\nSampler to generate the datasets for PDE and boundary conditions using a quisa-random sampling algorithm. You can call sample(pde, sampler, strategy) on it to generate all the datasets. See QuasiMonteCarlo.jl for available sampling algorithms. The default element type of the sampled data is Float64. The initial sampled data lives on GPU if PINN is. You will need manually move the data to GPU if you want to resample.\n\nArguments\n\npde_points: The number of points to sample for each PDE. If a single number is given, the same number of points will be sampled for each PDE. If a tuple of numbers is given, the number of points for each PDE will be the corresponding element in the tuple. The default is 100.\nbcs_points: The number of points to sample for each boundary condition. If a single number is given, the same number of points will be sampled for each boundary condition. If a tuple of numbers is given, the number of points for each boundary condition will be the corresponding element in the tuple. The default is pde_points.\n\nKeyword Arguments\n\nsampling_alg: The sampling algorithm to use. The default is SobolSample().\nresample: Whether to resample the data for each equation. The default is false, which can save a lot of memory if you are solving a large number of PDEs. In this case, pde_points has to be a integer. If you want to resample the data, you will need to manually move the data to GPU if you want to use GPU to solve the PDEs.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.RBF","page":"API Reference","title":"Sophon.RBF","text":"RBF(in_dims::Int, out_dims::Int, num_centers::Int=out_dims; sigma::AbstractFloat=0.2f0)\n\nNormalized Radial Basis Fuction Network.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.ScalarLayer","page":"API Reference","title":"Sophon.ScalarLayer","text":"ScalarLayer(connection::Function)\n\nReturn connection(scalar, x)\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.SplitFunction","page":"API Reference","title":"Sophon.SplitFunction","text":"SplitFunction(indices...)\n\nSplit the input along the first demision according to indices.\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.TriplewiseFusion","page":"API Reference","title":"Sophon.TriplewiseFusion","text":"TriplewiseFusion(connection, layers...)\n\n     u1                    u2\n        ↘                     ↘\nh1 → layer1 → connection → layer2 → connection\n        ↗                     ↗\n     v1                    v2\n\nArguments\n\nconnection: A functio takes 3 inputs and combines them.\nlayers: AbstractExplicitLayers or a Chain.\n\nInputs\n\nLayer behaves differently based on input type:\n\nA tripe of (h, u, v), where u and v itself are tuples of length N, the layers is also a tuple of length N. The computation is as follows\n\nfor i in 1:N\n    h = connection(layers[i](h), u[i], v[i])\nend\n\nA triple of (h, u, v), where u and v are AbstractArrays.\n\nfor i in 1:N\n    h = connection(layers[i](h), u, v)\nend\n\nParameters\n\nParameters of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nStates\n\nStates of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\n\n\n\n\n","category":"type"},{"location":"api/#Sophon.BACON-Tuple{Int64, Int64, Int64, Real}","page":"API Reference","title":"Sophon.BACON","text":"BACON(in_dims::Int, out_dims::Int, N::Int, period::Real; hidden_dims::Int, num_layers::Int)\n\nBand-limited Coordinate Networks (BACON) from [6]. Similar to FourierFilterNet but the frequcies are dicrete and nontrainable.\n\nTips: It is recommended to set period to be 1,2,π or 2π for better performance.\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.FourierAttention-Tuple{Int64, Int64, Function, Any}","page":"API Reference","title":"Sophon.FourierAttention","text":"FourierAttention(in_dims::Int, out_dims::Int, activation::Function, std;\n                 hidden_dims::Int=512, num_layers::Int=6, modes::NTuple)\nFourierAttention(in_dims::Int, out_dims::Int, activation::Function, frequencies;\n                 hidden_dims::Int=512, num_layers::Int=6, modes::NTuple)\n\nA model that combines FourierFeature and PINNAttention.\n\nx → [FourierFeature(x); x] → PINNAttention\n\nArguments\n\nin_dims: The input dimension.\nout_dims: The output dimension.\nactivation: The activation function.\nstd: See FourierFeature.\nfrequencies: See FourierFeature.\n\nKeyword Arguments\n\nhidden_dim: The hidden dimension of each hidden layer.\nnum_layers: The number of hidden layers.\n\nExamples\n\njulia> FourierAttention(3, 1, sin, (1 => 10, 10 => 10, 50 => 10); hidden_dims=10, num_layers=3)\nChain(\n    layer_1 = SkipConnection(\n        FourierFeature(3 => 60),\n        vcat\n    ),\n    layer_2 = PINNAttention(\n        H_net = Dense(63 => 10, sin),   # 640 parameters\n        U_net = Dense(63 => 10, sin),   # 640 parameters\n        V_net = Dense(63 => 10, sin),   # 640 parameters\n        fusion = TriplewiseFusion(\n            layers = (layer_1 = Dense(10 => 10, sin), layer_2 = Dense(10 => 10, sin), layer_3 = Dense(10 => 10, sin), layer_4 = Dense(10 => 10, sin)),  # 440 parameters\n        ),\n    ),\n    layer_3 = Dense(10 => 1),           # 11 parameters\n)         # Total: 2_371 parameters,\n          #        plus 90 states, summarysize 192 bytes\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.FourierFilterNet-Tuple{Int64, Int64}","page":"API Reference","title":"Sophon.FourierFilterNet","text":"FourierFilterNet(in_dims::Int, out_dims::Int; hidden_dims::Int, num_layers::Int,\n                 bandwidth::Real)\n\nMultiplicative filter network defined by\n\nbeginaligned\nz^(1) =gleft(x  theta^(1)right) \nz^(i+1) =left(W^(i) z^(i)+b^(i)right) circ sin left(omega^(i) x+phi^(i)right)right) \nf(x) =W^(k) z^(k)+b^(k)\nendaligned\n\nKeyword Arguments\n\nbandwidth: The maximum bandwidth of the network. The bandwidth is the sum of each filter's bandwidth.\n\nParameters\n\nParameters of the filters:\n\n    Wsim mathcalU(-fracωn fracωn) quad bsim mathcalU(-pi pi)\n\nwhere n is the number of filters.\n\nFor a periodic function with period P, the Fourier series in amplitude-phase form is\n\ns_N(x)=fraca_02+sum_n=1^Na_ncdot sin left( frac2piPnx+varphi _n right)\n\nWe have the following relation between the banthwidth and the parameters of the model:\n\nω = 2πB=frac2πNP\n\nwhere B is the bandwidth of the network.\n\nReferences\n\n[11]\n\n[6]\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.FourierNet-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function, Tuple{Vararg{T, N}} where {N, T}}} where {N, T<:Int64}","page":"API Reference","title":"Sophon.FourierNet","text":"FourierNet(ayer_sizes::NTuple, activation, modes::NTuple)\n\nA model that combines FourierFeature and FullyConnected.\n\nx → FourierFeature → FullyConnected → y\n\nArguments\n\nin_dims: The number of input dimensions.\nlayer_sizes: A tuple of hidden dimensions used to construct FullyConnected.\nactivation: The activation function used to construct FullyConnected.\nmodes: A tuple of modes used to construct FourierFeature.\n\nExamples\n\njulia> FourierNet((2, 30, 30, 1), sin, (1 => 10, 10 => 10, 50 => 10))\nChain(\n    layer_1 = FourierFeature(2 => 60),\n    layer_2 = Dense(60 => 30, sin),     # 1_830 parameters\n    layer_3 = Dense(30 => 30, sin),     # 930 parameters\n    layer_4 = Dense(30 => 1),           # 31 parameters\n)         # Total: 2_791 parameters,\n          #        plus 60 states, summarysize 112 bytes.\n\njulia> FourierNet((2, 30, 30, 1), sin, (1, 2, 3, 4))\nChain(\n    layer_1 = FourierFeature(2 => 16),\n    layer_2 = Dense(16 => 30, sin),     # 510 parameters\n    layer_3 = Dense(30 => 30, sin),     # 930 parameters\n    layer_4 = Dense(30 => 1),           # 31 parameters\n)         # Total: 1_471 parameters,\n          #        plus 4 states, summarysize 96 bytes.\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T<:Int64}","page":"API Reference","title":"Sophon.FullyConnected","text":"FullyConnected(layer_sizes::NTuple{N, Int}, activation; outermost = true,\n               init_weight=kaiming_uniform(activation),\n               init_bias=zeros32,\n               allow_fast_activation=false)\nFullyConnected(in_dims::Int, out_dims::Int, activation::Function;\n               hidden_dims::Int, num_layers::Int, outermost=true,\n               init_weight=kaiming_uniform(activation),\n               init_bias=zeros32,\n               allow_fast_activation=false)\n\nCreate fully connected layers.\n\nArguments\n\nlayer_sizes: Number of dimensions of each layer.\nhidden_dims: Number of hidden dimensions.\nnum_layers: Number of layers.\nactivation: Activation function.\n\nKeyword Arguments\n\noutermost: Whether to use activation function for the last layer. If false, the activation function is applied to the output of the last layer.\ninit_weight: Initialization method for the weights.\nallow_fast_activation: If true, then certain activations can be approximated with a faster version. The new activation function will be given by NNlib.fast_act(activation)\n\nExample\n\njulia> fc = FullyConnected((1, 12, 24, 32), relu)\nChain(\n    layer_1 = Dense(1 => 12, relu),     # 24 parameters\n    layer_2 = Dense(12 => 24, relu),    # 312 parameters\n    layer_3 = Dense(24 => 32),          # 800 parameters\n)         # Total: 1_136 parameters,\n          #        plus 0 states, summarysize 48 bytes.\n\njulia> fc = FullyConnected(1, 10, relu; hidden_dims=20, num_layers=3)\nChain(\n    layer_1 = Dense(1 => 20, relu),     # 40 parameters\n    layer_2 = Dense(20 => 20, relu),    # 420 parameters\n    layer_3 = Dense(20 => 20, relu),    # 420 parameters\n    layer_4 = Dense(20 => 10),          # 210 parameters\n)         # Total: 1_090 parameters,\n          #        plus 0 states, summarysize 64 bytes.\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.ResNet-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T<:Int64}","page":"API Reference","title":"Sophon.ResNet","text":"ResNet(layer_sizes::NTuple{N, Int}, activation; outermost=true,\n               init_weight=kaiming_uniform(activation),\n               init_bias=zeros32,\n               allow_fast_activation=false)\nResNet(in_dims::Int, out_dims::Int, activation::Function;\n               hidden_dims::Int, num_layers::Int, outermost=true,\n               init_weight=kaiming_uniform(activation),\n               init_bias=zeros32,\n               allow_fast_activation=false)\n\nCreate fully connected layers.\n\nArguments\n\nlayer_sizes: Number of dimensions of each layer.\nhidden_dims: Number of hidden dimensions.\nnum_layers: Number of layers.\nactivation: Activation function.\n\nKeyword Arguments\n\noutermost: Whether to use activation function for the last layer. If false, the activation function is applied to the output of the last layer.\ninit_weight: Initialization method for the weights.\nallow_fast_activation: If true, then certain activations can be approximated with a faster version. The new activation function will be given by NNlib.fast_act(activation)\n\nExample\n\njulia> ResNet((1, 12, 24, 32), relu)\nChain(\n    layer_1 = Dense(1 => 12, relu),     # 24 parameters\n    layer_2 = SkipConnection(\n        Dense(12 => 24, relu),          # 312 parameters\n        +\n    ),\n    layer_3 = Dense(24 => 32),          # 800 parameters\n)         # Total: 1_136 parameters,\n          #        plus 0 states, summarysize 48 bytes.\n\njulia> ResNet(1, 10, relu; hidden_dims=20, num_layers=3)\nChain(\n    layer_1 = Dense(1 => 20, relu),     # 40 parameters\n    layer_2 = SkipConnection(\n        Dense(20 => 20, relu),          # 420 parameters\n        +\n    ),\n    layer_3 = SkipConnection(\n        Dense(20 => 20, relu),          # 420 parameters\n        +\n    ),\n    layer_4 = Dense(20 => 10),          # 210 parameters\n)         # Total: 1_090 parameters,\n          #        plus 0 states, summarysize 64 bytes.\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.Sine-Union{Tuple{Pair{T, T}}, Tuple{T}} where T<:Int64","page":"API Reference","title":"Sophon.Sine","text":"Sine(in_dims::Int, out_dims::Int; omega::Real)\n\nSinusoidal layer.\n\nExample\n\ns = Sine(2, 2; omega=30.0f0) # first layer\ns = Sine(2, 2) # hidden layer\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.Siren-Tuple{Int64, Int64}","page":"API Reference","title":"Sophon.Siren","text":"Siren(in_dims::Int, out_dims::Int; hidden_dims::Int, num_layers::Int, omega=30.0f0,\n      init_weight=nothing))\nSiren(layer_sizes::Int...; omega=30.0f0, init_weight=nothing)\n\nSinusoidal Representation Network.\n\nKeyword Arguments\n\nomega: The ω₀ used for the first layer.\ninit_weight: The initialization algorithm for the weights of the input layer. Note that all hidden layers use kaiming_uniform as the initialization algorithm. The default is\n    Wsim mathcalUleft(-fracomegafan_in fracomegafan_inright)\n\nExamples\n\njulia> Siren(2, 32, 32, 1; omega=5.0f0)\nChain(\n    layer_1 = Dense(2 => 32, sin),      # 96 parameters\n    layer_2 = Dense(32 => 32, sin),     # 1_056 parameters\n    layer_3 = Dense(32 => 1),           # 33 parameters\n)         # Total: 1_185 parameters,\n          #        plus 0 states, summarysize 48 bytes.\n\njulia> Siren(3, 1; hidden_dims=20, num_layers=3)\nChain(\n    layer_1 = Dense(3 => 20, sin),      # 80 parameters\n    layer_2 = Dense(20 => 20, sin),     # 420 parameters\n    layer_3 = Dense(20 => 20, sin),     # 420 parameters\n    layer_4 = Dense(20 => 1),           # 21 parameters\n)         # Total: 941 parameters,\n          #        plus 0 states, summarysize 64 bytes.\n\n# Use your own initialization algorithm for the input layer.\njulia> init_weight(rng::AbstractRNG, out_dims::Int, in_dims::Int) = randn(rng, Float32, out_dims, in_dims) .* 2.5f0\njulia> chain = Siren(2, 1; num_layers = 4, hidden_dims = 50, init_weight = init_weight)\n\nReference\n\n[1]\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.discretize-Tuple{Any, PINN, Sophon.PINNSampler, Sophon.AbstractTrainingAlg}","page":"API Reference","title":"Sophon.discretize","text":" discretize(pde_system::PDESystem, pinn::PINN, sampler::PINNSampler,\n                strategy::AbstractTrainingAlg; derivative=finitediff,\n                additional_loss)\n\nConvert the PDESystem into an OptimizationProblem. You will have access to each loss function Sophon.residual_function_1, Sophon.residual_function_2... after calling this function.\n\n\n\n\n\n","category":"method"},{"location":"api/#Sophon.gaussian","page":"API Reference","title":"Sophon.gaussian","text":"gaussian(x, a=0.2)\n\nThe Gaussian activation function.\n\ne^frac- x^22a^2\n\nReference\n\n[2]\n\n\n\n\n\n","category":"function"},{"location":"api/#Sophon.stan","page":"API Reference","title":"Sophon.stan","text":"stan()\n\nSelf-scalable Tanh.\n\nsigma(x^i) = tanh(x^i) + beta^i * x^i * tanh(x^i)\n\nReference\n\n[12]\n\n\n\n\n\n","category":"function"},{"location":"tutorials/convection/#D-Convection-Equation","page":"1D Convection Equation","title":"1D Convection Equation","text":"","category":"section"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"Consider the following 1D-convection equation with periodic boundary conditions.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"beginaligned\nfracpartial upartial t+c fracpartial upartial x=0 x in01 t in01 \nu(x 0)=sin(2pi x) \nendaligned","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"First we define the PDE.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"using ModelingToolkit, Sophon, IntervalSets, CairoMakie\nusing Optimization, OptimizationOptimJL, Zygote\n\n@parameters x, t\n@variables u(..)\nDₜ = Differential(t)\nDₓ = Differential(x)\n\nc = 6\neq = Dₜ(u(x,t)) + c * Dₓ(u(x,t)) ~ 0\nu_analytic(x,t) = sinpi(2*(x-c*t))\n\ndomains = [x ∈ 0..1, t ∈ 0..1]\n\nbcs = [u(x,0) ~ u_analytic(x,0)]\n\n@named convection = PDESystem(eq, bcs, domains, [x,t], [u(x,t)])","category":"page"},{"location":"tutorials/convection/#Imposing-periodic-boundary-conditions","page":"1D Convection Equation","title":"Imposing periodic boundary conditions","text":"","category":"section"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"We will use BACON to impose the boundary conditions. To this end, we simply set period to be one.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"chain = BACON(2, 1, 8, 1; hidden_dims = 32, num_layers=4)","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"note: Note\nFor demonstration purposes, the model is also periodic in time","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"sampler = QuasiRandomSampler(500, 100) # data points\nstrategy = NonAdaptiveTraining(1 , 500) # weights\npinn = PINN(chain)\n\nprob = Sophon.discretize(convection, pinn, sampler, strategy) \n\n@showprogress res = Optimization.solve(prob, BFGS(); maxiters = 1000)","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"Let's visualize the result.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"phi = pinn.phi\n\nxs, ts= [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_pred = [sum(phi([x,t],res.u)) for x in xs, t in ts]\nu_real = u_analytic.(xs,ts')\nfig, ax, hm = heatmap(ts, xs, u_pred', axis=(xlabel=\"t\", ylabel=\"x\", title=\"c = $c\"))\nax2, hm2 = heatmap(fig[1,end+1], ts,xs, abs.(u_pred' .- u_real'), axis = (xlabel=\"t\", ylabel=\"x\", title=\"Absolute error\"))\nColorbar(fig[:, end+1], hm2)\ndisplay(fig)\nsave(\"convection.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"(Image: )","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"We can verify that our model is indeed, periodic.","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"xs, ts= [infimum(d.domain):0.01:supremum(d.domain)*2 for d in domains]\nu_pred = [sum(phi([x,t],res.u)) for x in xs, t in ts]\nfig, ax, hm = heatmap(ts, xs, u_pred', axis=(xlabel=\"t\", ylabel=\"x\", title=\"c = $c\"))\ndisplay(fig)\nsave(\"convection2.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/convection/","page":"1D Convection Equation","title":"1D Convection Equation","text":"(Image: )","category":"page"},{"location":"tutorials/helmholtz/#Helmholtz-equation","page":"2D Helmholtz Equation","title":"Helmholtz equation","text":"","category":"section"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"Let us consider the Helmholtz equation in two space dimensions","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"beginaligned\nDelta u(x y)+k^2 u(x y)=q(x y) quad(x y) in Omega=(-11)^2 \nu(x y)=0 quad(x y) in partial Omega\nendaligned","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"where ","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"q(x y)=-left(a_1 piright)^2 sin left(a_1 pi xright) sin left(a_2 pi yright)-left(a_2 piright)^2 sin left(a_1 pi xright) sin left(a_2 pi yright)+k^2 sin left(a_1 pi xright) sin left(a_2 pi yright)","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"The exact solution is u(xy)=sina_1pi xsina_2pi y. We chose k=1 a_1 = 1 and a_2 = 4.","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"using ModelingToolkit, IntervalSets, Sophon, Lux, Zygote\nusing Optimization, OptimizationOptimJL\n\n@parameters x,y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\na1 = 1\na2 = 4\nk = 1\n\nq(x,y) = -(a1*π)^2 * sin(a1*π*x) * sin(a2*π*y) - (a2*π)^2 * sin(a1*π*x) * sin(a2*π*y) + k^2 * sin(a1*π*x) * sin(a2*π*y)\neq = Dxx(u(x,y)) + Dyy(u(x,y)) + k^2 * u(x,y) ~ q(x,y)\ndomains = [x ∈ Interval(-1,1), y ∈ Interval(-1,1)]\nbcs = [u(-1,y) ~ 0, u(1,y) ~ 0, u(x, -1) ~ 0, u(x, 1) ~ 0]\n\n@named helmholtz = PDESystem(eq, bcs, domains, [x,y], [u(x,y)])","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"Note that the boundary conditions are compatible with periocity, which allows us to apply BACON.","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"chain = BACON(2, 1, 5, 2; hidden_dims = 32, num_layers=5)\npinn = PINN(chain) # call `gpu` on it if you want to use gpu\nsampler = QuasiRandomSampler(300, 100)  \nstrategy = NonAdaptiveTraining()\n\nprob = Sophon.discretize(helmholtz, pinn, sampler, strategy) \n\n@showprogress res = Optimization.solve(prob, BFGS(); maxiters=1000)","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"Let's plot the result.","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"phi = pinn.phi\n\nxs, ys= [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_analytic(x,y) = sinpi(a1*x)*sinpi(a2*y)\nu_real = [u_analytic(x,y) for x in xs, y in ys]\n\nphi_cpu = cpu(phi) # in case you are using GPU\nps_cpu = cpu(res.u)\nu_pred = [sum(phi_cpu(([x,y]), ps_cpu)) for x in xs, y in ys]\n\nusing CairoMakie\naxis = (xlabel=\"x\", ylabel=\"y\", title=\"Analytical Solution\")\nfig, ax1, hm1 = heatmap(xs, ys, u_real, axis=axis)\nColorbar(fig[:, end+1], hm1)\nax2, hm2= heatmap(fig[1, end+1], xs, ys, u_pred, axis= merge(axis, (;title = \"Prediction\")))\nColorbar(fig[:, end+1], hm2)\nax3, hm3 = heatmap(fig[1, end+1], xs, ys, abs.(u_pred-u_real), axis= merge(axis, (;title = \"Absolute Error\")))\nColorbar(fig[:, end+1], hm3)\nfig\nsave(\"helmholtz.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/helmholtz/","page":"2D Helmholtz Equation","title":"2D Helmholtz Equation","text":"(Image: )","category":"page"},{"location":"tutorials/sod/#Sod-shock-tube-with-adaptive-weights","page":"SOD Shock Tube Problem","title":"Sod shock tube with adaptive weights","text":"","category":"section"},{"location":"tutorials/sod/","page":"SOD Shock Tube Problem","title":"SOD Shock Tube Problem","text":"This example showcases how to use adaptive weights in Sophon. ","category":"page"},{"location":"tutorials/sod/","page":"SOD Shock Tube Problem","title":"SOD Shock Tube Problem","text":"using Optimization, OptimizationOptimJL, Plots, Zygote\nusing ModelingToolkit, IntervalSets\nusing Sophon\nusing ChainRulesCore\n\n@parameters t, x\n@variables u(..), ρ(..), p(..)\nDₓ = Differential(x)\nDₜ = Differential(t)\n\nu₀(x) = 0.0\nρ₀(x) = ifelse(x < 0.5, 1.0, 0.125)\n@register ρ₀(x)\np₀(x) = ifelse(x < 0.5, 1.0, 0.1)\n@register p₀(x)\n\nbcs = [ρ(0, x) ~ ρ₀(x), u(0, x) ~ u₀(x), p(0, x) ~ p₀(x), u(t, 0) ~ 0.0, u(t, 1) ~ 0.0]\n\nγ = 1.4\nE(t, x) = p(t, x) / (γ - 1) + 0.5 * ρ(t, x) * abs2(u(t, x))\n\neqs = [\n    Dₜ(ρ(t, x)) + Dₓ(ρ(t, x) * u(t, x)) ~ 0.0,\n    Dₜ(ρ(t, x) * u(t, x)) + Dₓ(ρ(t, x) * u(t, x) * u(t, x) + p(t, x)) ~ 0.0,\n    Dₜ(E(t, x)) + Dₓ(u(t, x) * (E(t, x) + p(t, x))) ~ 0.0,\n]\n\nt_min, t_max = 0.0, 0.2\nx_min, x_max = 0.0, 1.0\ndomains = [t ∈ Interval(t_min, t_max), x ∈ Interval(x_min, x_max)]\n\n@named pde_system = PDESystem(eqs, bcs, domains, [t, x], [u(t, x), ρ(t, x), p(t, x)])","category":"page"},{"location":"tutorials/sod/","page":"SOD Shock Tube Problem","title":"SOD Shock Tube Problem","text":"pinn = PINN(u=FullyConnected(2, 1, tanh; num_layers=4, hidden_dims=16),\n            ρ=FullyConnected(2, 1, tanh; num_layers=4, hidden_dims=16),\n            p=FullyConnected(2, 1, tanh; num_layers=4, hidden_dims=16))\n\nsampler = QuasiRandomSampler(1000, 400)\n\nfunction pde_weights(phi, x, θ)\n    ux = Sophon.finitediff(phi.u, x, θ.u, 1, 1)\n    ρx = Sophon.finitediff(phi.ρ, x, θ.ρ, 1, 1)\n    px = Sophon.finitediff(phi.p, x, θ.p, 1, 1)\n    d = ux .+ ρx .+ px\n\n    return ChainRulesCore.@ignore_derivatives inv.(0.2 .* abs.(d) .+ 1)\nend\n\nstrategy = AdaptiveTraining(pde_weights, Returns(10))\nprob = Sophon.discretize(pde_system, pinn, sampler, strategy)\n\n@showprogress res = Optimization.solve(prob, BFGS(); maxiters=1000)\n\nθ = res.u\nphi = pinn.phi\nxs = x_min:0.01:x_max\n\nphi = pinn.phi\np1 = plot(xs, [first(phi.u([t_max, x], θ.u)) for x in xs]; label=\"u(t=1,x)\")\np2 = plot!(xs, [first(phi.ρ([t_max, x], θ.ρ)) for x in xs]; label=\"ρ(t=1,x)\")\np3 = plot!(xs, [first(phi.p([t_max, x], θ.p)) for x in xs]; label=\"p(t=1,x)\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Sophon","category":"page"},{"location":"#Sophon","page":"Home","title":"Sophon","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Sophon.jl is a Julia package for solving partial differential equations (PDEs) using physics-informed neural networks (PINNs). We will go through the steps to solve the nonlinear Schrödinger equation using Sophon.jl.","category":"page"},{"location":"#Step-1:-Import-the-required-packages","page":"Home","title":"Step 1: Import the required packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using ModelingToolkit, IntervalSets, Sophon, CairoMakie\nusing Optimization, OptimizationOptimJL","category":"page"},{"location":"#Step-2:-Define-the-PDE-system","page":"Home","title":"Step 2: Define the PDE system","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In this step, we define the PDE system for the nonlinear Schrödinger equation using ModelingToolkit.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@parameters x,t\n@variables u(..), v(..)\nDₜ = Differential(t)\nDₓ² = Differential(x)^2\n\neqs=[Dₜ(u(x,t)) ~ -Dₓ²(v(x,t))/2 - (abs2(v(x,t)) + abs2(u(x,t))) * v(x,t),\n     Dₜ(v(x,t)) ~  Dₓ²(u(x,t))/2 + (abs2(v(x,t)) + abs2(u(x,t))) * u(x,t)]\n\nbcs = [u(x, 0.0) ~ 2sech(x),\n       v(x, 0.0) ~ 0.0,\n       u(-5.0, t) ~ u(5.0, t),\n       v(-5.0, t) ~ v(5.0, t)]\n\ndomains = [x ∈ Interval(-5.0, 5.0),\n           t ∈ Interval(0.0, π/2)]\n\n@named pde_system = PDESystem(eqs, bcs, domains, [x,t], [u(x,t),v(x,t)])","category":"page"},{"location":"","page":"Home","title":"Home","text":"The @parameters macro defines the parameters of the PDE system, and the @variables macro defines the dependent variables. We use Differential to define the derivatives with respect to time and space. The eqs array defines the equations in the PDE system. The bcs array defines the boundary conditions. The domains array defines the spatial and temporal domains of the PDE system. Finally, we use the @named macro to give a name to the PDE system.","category":"page"},{"location":"#Step-3:-Define-the-neural-network-architecture","page":"Home","title":"Step 3: Define the neural network architecture","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Next, we define the physics-informed neural network (PINN) using Sophon.jl. In this example, we will use a Siren network with 2 sine layers and 1 cosine layer for each variable, and 16 hidden dimensions per layer. We will use 4 layers for both variables, and set the frequency parameter omega to 1.0.","category":"page"},{"location":"","page":"Home","title":"Home","text":"pinn = PINN(u = Siren(2,1; hidden_dims=16,num_layers=4, omega = 1.0),\n            v = Siren(2,1; hidden_dims=16,num_layers=4, omega = 1.0))            ","category":"page"},{"location":"","page":"Home","title":"Home","text":"We define a physics-informed neural network (PINN) with the pinn variable. The PINN macro takes a dictionary that maps the dependent variables to their corresponding neural network architecture. In this case, we use the Siren architecture for both u and v with 2 input dimensions, 1 output dimension, 16 hidden dimensions, and 4 layers. We also set the frequency of the sine activation functions to 1.0.","category":"page"},{"location":"#Step-4:-Create-a-QuasiRandomSampler-object","page":"Home","title":"Step 4: Create a QuasiRandomSampler object","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here, we create a QuasiRandomSampler object with 500 sample points, where the first argument corresponds to the number of data points for each equation, and the second  argument corresponds to the number of data points for each boundary condition.","category":"page"},{"location":"","page":"Home","title":"Home","text":"sampler = QuasiRandomSampler(500, (200,200,20,20))","category":"page"},{"location":"#Step-5:-Define-a-training-strategy","page":"Home","title":"Step 5: Define a training strategy","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here, we use a NonAdaptiveTraining strategy with 1 as the weight of all equations, and (10,10,1,1) for the four boundary conditions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"strategy = NonAdaptiveTraining(1,(10,10,1,1))","category":"page"},{"location":"#Step-6:-Discretize-the-PDE-system-using-Sophon","page":"Home","title":"Step 6: Discretize the PDE system using Sophon","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"prob = Sophon.discretize(pde_system, pinn, sampler, strategy)","category":"page"},{"location":"#Step-7:-Solve-the-optimization-problem","page":"Home","title":"Step 7: Solve the optimization problem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"res = Optimization.solve(prob, BFGS(); maxiters=2000)","category":"page"},{"location":"tutorials/waveinverse2/#Inverse-problem-for-the-wave-equation-with-unknown-velocity-field","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"","category":"section"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"We are going to sovle the wave equation.","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"using Sophon, ModelingToolkit, IntervalSets\nusing Optimization, OptimizationOptimJL, Zygote\n\n@parameters x, t\n@variables u(..), c(..)\n\nDₜ = Differential(t)\nDₜ² = Differential(t)^2\nDₓ² = Differential(x)^2\n\ns(x,t) = abs2(x) * sin(x) * cos(t)\n\neq = Dₜ²(u(x,t)) ~ c(x) * Dₓ²(u(x,t)) + s(x,t)\n\nbcs = [u(x, 0) ~ sin(x),\n       Dₜ(u(x, 0)) ~ 0,\n       u(0, t) ~ 0,\n       u(1, t) ~ sin(1) * cos(t)]\n\ndomains = [t ∈ Interval(0.0, 1.0),\n           x ∈ Interval(0.0, 1.0)]\n\n@named wave = PDESystem(eq, bcs, domains, [t,x], [u(x,t),c(x)])","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"Here the velocity field c(x) is unknown, we will approximate it with a neural network.","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"pinn = PINN(u = FullyConnected((2,16,16,16,1), sin),\n            c = FullyConnected((1,16,16,1), tanh))\n\nsampler = QuasiRandomSampler(500,100)\nstrategy = NonAdaptiveTraining(1, (10,10,1,1))","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"Next we generate some data of u(xt). Here we place two sensors at x=01 and x=05.","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"ū(x,t) = sin(x) * cos(t)\n\nx_data = hcat(fill(0.1, 1, 50), fill(0.5, 1, 50))\nt_data = repeat(range(0.0, 1.0, length = 50),2)'\ninput_data = [x_data; t_data]\n\nu_data = ū.(x_data, t_data)","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"Finally we construct the inverse problem and solve it.","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"additional_loss(phi, θ) = sum(abs2, phi.u(input_data, θ.u) .- u_data)\n\nprob = Sophon.discretize(wave, pinn, sampler, strategy; additional_loss=additional_loss)\n\n@showprogress res = Optimization.solve(prob, BFGS(), maxiters=1000)","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"Let's visualize the predictted solution and inferred velocity","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"using CairoMakie\n\nts = range(0, 1; length=100)\nxs = range(0, 1; length=100)\n\nu_pred = [pinn.phi.u([x, t], res.u.u)[1] for x in xs, t in ts]\nc_pred = [pinn.phi.c([x], res.u.c)[1] for x in xs]\n\nu_true = [ū(x, t) for x in xs, t in ts]\nc_true = 1 .+ abs2.(xs) |> vec\n\naxis = (xlabel=\"x\", ylabel=\"t\", title=\"Analytical Solution\")\nfig, ax1, hm1 = heatmap(xs, ts, u_true, axis=axis; colormap=:jet)\nax2, hm2= heatmap(fig[1, end+1], xs, ts, u_pred, axis= merge(axis, (;title = \"Prediction\")); colormap=:jet)\nax3, hm3 = heatmap(fig[1, end+1], xs, ts, abs.(u_true .- u_pred), axis= merge(axis, (;title = \"Absolute Error\")); colormap=:jet)\nColorbar(fig[:, end+1], hm3)\nfig\nsave(\"sol.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"(Image: )","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"fig, ax = lines(xs, c_pred)\nlines!(ax, xs, c_true)\nfig\nsave(\"velocity.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/waveinverse2/","page":"Inverse problem for the wave equation with unknown velocity field","title":"Inverse problem for the wave equation with unknown velocity field","text":"(Image: )","category":"page"},{"location":"tutorials/L_shape/#Poisson-equation-over-an-L-shaped-domain","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"","category":"section"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"The example is taken from here. We showcase define a PDE on an L-shaped domain","category":"page"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"using ModelingToolkit, DomainSets, Optimization, OptimizationOptimJL, Zygote\nusing DomainSets: ×\nusing Sophon\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\neq = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -1.0\neqs = [eq => (-1..0) × (-1..0),\n       eq => (-1..0) × (0..1),\n       eq => (0..1) × (-1..0)]\n\nbc = u(x,y) ~ 0.0\nboundaries = [(-1 .. -1) × (-1..1),\n              (-1..0) × (1..1),\n              (0..0) × (0..1),\n              (0..1) × (0..0),\n              (1..1) × (-1..0),\n              (-1..1) × (-1 .. -1)]\n\nbcs = [bc => boundary for boundary in boundaries]\n\npde_system = Sophon.PDESystem(eqs, bcs, [x,y], [u(x,y)])","category":"page"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"chain = FullyConnected((2,16,16,16,1), tanh)\npinn = PINN(chain)\nsampler = QuasiRandomSampler(300, 30)\nstrategy = NonAdaptiveTraining()\n\nprob = Sophon.discretize(pde_system, pinn, sampler, strategy)\n\n@showprogress res = Optimization.solve(prob, BFGS(); maxiters=1000)\n\nusing CairoMakie\n\nxs = -1:0.01:1\nys = -1:0.01:1\n\nu_pred = [ifelse(x>0.0 && y>0.0, NaN, pinn.phi([x,y], res.u)[1]) for x in xs, y in ys]\nfig, ax, hm = heatmap(xs, ys, u_pred, colormap=:jet)\nColorbar(fig[:, end+1], hm)\nfig\nsave(\"Lshape.png\", fig); nothing # hide","category":"page"},{"location":"tutorials/L_shape/","page":"Poisson equation over an L-shaped domain","title":"Poisson equation over an L-shaped domain","text":"(Image: )","category":"page"}]
}
